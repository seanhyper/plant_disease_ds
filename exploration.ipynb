{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import subprocess\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import matplotlib.image as mpimg\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import seaborn as sns\n",
    "\n",
    "from classification_utils import *\n",
    "from train_classification import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> This Code creates DataFrame with the columns:\n",
    "- classfication (name of the image folder)\n",
    "- image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Path to the dataset directory\n",
    "dataset_dir = 'data/PlantVillage/'\n",
    "\n",
    "# Function to load images and labels using OpenCV\n",
    "def load_dataset(dataset_path):\n",
    "    data_dict = {}\n",
    "    # Walk through the dataset directory\n",
    "    for root, dirs, files in os.walk(dataset_path):\n",
    "        for dir_name in dirs:\n",
    "            # Initialize an empty list for each subfolder\n",
    "            data_dict[dir_name] = []\n",
    "\n",
    "            # Get the full path to the subfolder\n",
    "            subfolder_path = os.path.join(root, dir_name)\n",
    "\n",
    "            # Iterate through the files in the subfolder\n",
    "            for file in os.listdir(subfolder_path):\n",
    "                if file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                    # Construct the full path to the image\n",
    "                    image_path = os.path.join(subfolder_path, file)\n",
    "\n",
    "                    # Load the image using OpenCV\n",
    "                    image = cv2.imread(image_path)\n",
    "                    # Convert from BGR to RGB color space\n",
    "                    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "                    # Append the image to the list associated with the subfolder name\n",
    "                    data_dict[dir_name].append(image)\n",
    "    data = [(classification, path) for classification, paths in data_dict.items() for path in paths]\n",
    "    df = pd.DataFrame(data, columns=['classification', 'image'])\n",
    "    return df\n",
    "\n",
    "# Load your dataset\n",
    "df = load_dataset(dataset_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Lets keep the tomatos images only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in df.classification.unique():\n",
    "    if 'tomato' in label.lower():\n",
    "        print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tomato_data = df.loc[df['classification'].str.contains('Tomato')]\n",
    "tomato_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Sample from each classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_images_per_class = 4\n",
    "images_to_display = []\n",
    "titles = []\n",
    "for label in tomato_data.classification.unique():\n",
    "    label_sample = tomato_data.loc[tomato_data.classification == label].sample(num_images_per_class).image.to_list()\n",
    "    images_to_display += label_sample\n",
    "    titles += [label]*num_images_per_class\n",
    "\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(nrows=len(tomato_data.classification.unique()), ncols=num_images_per_class, figsize=(15, 20))  \n",
    "for ax, image, title in zip(axes.flatten(), images_to_display, titles):\n",
    "    ax.imshow(image)\n",
    "    ax.set_title(title)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax, fig = plt.subplots(figsize=(15,7))\n",
    "sns.barplot(tomato_data.classification.value_counts().reset_index(), y='index', x='classification')\n",
    "plt.yticks(fontsize=14)\n",
    "plt.title(\"Number of images per classification\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Check weather the images resolution are unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tomato_data['image_shape'] = tomato_data.image.apply(lambda x: x.shape)\n",
    "    \n",
    "print(f\"The image resolution we have in the dataset are : {tomato_data.image_shape.unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmented_data_root_folder = 'data/segmented_data/'\n",
    "number_of_images_to_segment = 300\n",
    "\n",
    "for root, dirs, files in os.walk(dataset_dir):\n",
    "    for dir_name in dirs:\n",
    "        if 'tomato' in dir_name.lower():\n",
    "            subfolder_path = os.path.join(root, dir_name)\n",
    "            destination_directory = os.path.join(segmented_data_root_folder, dir_name)\n",
    "            os.makedirs(destination_directory, exist_ok=True)\n",
    "            \n",
    "            dir_files = os.listdir(subfolder_path)\n",
    "            random_files = random.sample(dir_files, min(number_of_images_to_segment, len(dir_files)))\n",
    "        else: continue\n",
    "        for file in random_files:\n",
    "            if file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                image_path = os.path.join(subfolder_path, file)\n",
    "                command = [\n",
    "                        \"python3\", \"leaf-image-segmentation/segment.py\",\n",
    "                        \"-d\", destination_directory,\n",
    "                        \"-f\", \"flood\",  \n",
    "                        image_path\n",
    "                    ]\n",
    "                subprocess.run(command)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmented_data = load_dataset('data/segmented_data/')\n",
    "segmented_data_dict = [(classification, image) for classification, images in segmented_data.items() for image in images]\n",
    "segmented_data_df = pd.DataFrame(segmented_data_dict, columns=['classification', 'image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_images_per_class = 4\n",
    "images_to_display = []\n",
    "titles = []\n",
    "for label in segmented_data_df.classification.unique():\n",
    "    label_sample = segmented_data_df.loc[segmented_data_df.classification == label].sample(num_images_per_class).image.to_list()\n",
    "    images_to_display += label_sample\n",
    "    titles += [label]*num_images_per_class\n",
    "\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(nrows=len(segmented_data_df.classification.unique()), ncols=num_images_per_class, figsize=(15, 20))  \n",
    "for ax, image, title in zip(axes.flatten(), images_to_display, titles):\n",
    "    ax.imshow(image)\n",
    "    ax.set_title(title)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_rgb_to_hsv(rgb_image):\n",
    "    # Convert the RGB image to HSV\n",
    "    return cv2.cvtColor(rgb_image, cv2.COLOR_RGB2HSV)\n",
    "\n",
    "# Apply the conversion function to each image in the DataFrame\n",
    "segmented_data_df['hsv_image'] = segmented_data_df['image'].apply(convert_rgb_to_hsv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(segmented_data_df.hsv_image.iloc[600])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(segmented_data_df.hsv_image.iloc[600])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmented_data_df.hsv_image.iloc.reshape(-1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "pixels = segmented_data_df.hsv_image.iloc[0].reshape(-1, 3)\n",
    "kmeans = KMeans(n_clusters=3)\n",
    "kmeans.fit(pixels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tomato Disease Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "b1 = 0.95\n",
    "b2 = 0.99\n",
    "num_epochs = 20\n",
    "train_data ,val_data, test_data = get_dataloader(dataset_dir = 'data/PlantVillage/' , target_count = 2200 ,batch_size = 75 ,show = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to save the model weights\n",
    "weights_folder = 'weights_classification'\n",
    "if not os.path.exists(weights_folder):\n",
    "    os.makedirs(weights_folder)\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    print (\"MPS device not found.\")\n",
    "print(device)\n",
    "beta = (b1, b2)\n",
    "model = CNN(num_classes=10)\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, betas=beta)\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "for epoch in tqdm(range(num_epochs), desc=\"Training\"):\n",
    "    train_loss, train_accuracy = train(model, train_data, criterion, optimizer, device)\n",
    "    val_loss, val_accuracy = validate(model, val_data, criterion, device)\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "\n",
    "    model_weights_path = os.path.join(weights_folder, f\"epoch_{epoch+1}_weights.pth\")\n",
    "    torch.save(model.state_dict(), model_weights_path)\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], lr={lr}, beta={beta}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = len(train_losses)\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, epochs + 1), train_losses, label='Train Loss')\n",
    "plt.plot(range(1, epochs + 1), val_losses, label='Validation Loss')\n",
    "plt.title(f'Loss (lr={lr}, beta={beta})')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, epochs + 1), train_accuracies, label='Train Accuracy')\n",
    "plt.plot(range(1, epochs + 1), val_accuracies, label='Validation Accuracy')\n",
    "plt.title(f'Accuracy (lr={lr}, beta={beta})')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'plots/best_train.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_epoch = 16\n",
    "model = CNN(num_classes=10)\n",
    "model.to(device)\n",
    "weights_path = f\"weights_classification/epoch_{best_epoch}_weights.pth\"\n",
    "\n",
    "# Load the weights\n",
    "model.load_state_dict(torch.load(weights_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create an empty list to store combined data\n",
    "combined_data_list = []\n",
    "\n",
    "# Iterate through the test_data\n",
    "for images, labels in test_data:\n",
    "    # Make predictions (you need to adjust this part based on how you make predictions)\n",
    "    predictions = model(images.to(device))\n",
    "    _, predicted_labels = torch.max(predictions, 1)\n",
    "\n",
    "    # Append data to the combined list\n",
    "    for real_label, pred_label in zip(labels, predicted_labels):\n",
    "        combined_data_list.append({'real_label': real_label.item(),\n",
    "                                   'pred_label': pred_label.item(),\n",
    "                                   'train': False})  # This is test data\n",
    "\n",
    "# Iterate through the train_data\n",
    "for images, labels in train_data:\n",
    "    # Make predictions (you need to adjust this part based on how you make predictions)\n",
    "    predictions = model(images.to(device))\n",
    "    _, predicted_labels = torch.max(predictions, 1)\n",
    "\n",
    "    # Append data to the combined list\n",
    "    for real_label, pred_label in zip(labels, predicted_labels):\n",
    "        combined_data_list.append({'real_label': real_label.item(),\n",
    "                                   'pred_label': pred_label.item(),\n",
    "                                   'train': True})  # This is train data\n",
    "\n",
    "# Create a DataFrame from the combined list\n",
    "combined_df = pd.DataFrame(combined_data_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.to_csv('classification/data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mixed_image_per_class(model, data_loader, num_images_per_class=5):\n",
    "    mixed_images = {}\n",
    "    \n",
    "    # Initialize the dictionary for each class\n",
    "    for class_idx in range(10):  \n",
    "        mixed_images[class_idx] = {'correct': [], 'incorrect': []}\n",
    "    \n",
    "    # Iterate through each batch in the data loader\n",
    "    for images, labels in data_loader:\n",
    "        # Make predictions\n",
    "        predictions = model(images.to(device))\n",
    "        _, predicted_labels = torch.max(predictions, 1)\n",
    "        \n",
    "        # Iterate through the batch\n",
    "        for image, true_label, pred_label in zip(images, labels, predicted_labels):\n",
    "            class_idx = true_label.item()\n",
    "            if true_label == pred_label:\n",
    "                # Append to correct predictions if less than num_images_per_class\n",
    "                if len(mixed_images[class_idx]['correct']) < num_images_per_class:\n",
    "                    mixed_images[class_idx]['correct'].append(image)\n",
    "            else:\n",
    "                # Append to incorrect predictions if less than num_images_per_class\n",
    "                if len(mixed_images[class_idx]['incorrect']) < num_images_per_class:\n",
    "                    mixed_images[class_idx]['incorrect'].append(image)\n",
    "            \n",
    "            # # Break if we have enough examples for both correct and incorrect predictions for the current class\n",
    "            # if len(mixed_images[class_idx]['correct']) >= num_images_per_class and len(mixed_images[class_idx]['incorrect']) >= num_images_per_class:\n",
    "            #     break\n",
    "                \n",
    "    return mixed_images\n",
    "\n",
    "# Example usage\n",
    "mixed_images_dict = create_mixed_image_per_class(model, test_data, num_images_per_class=5)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "def create_combined_image_per_class(mixed_images_dict):\n",
    "    combined_images = []\n",
    "    \n",
    "    for class_idx in mixed_images_dict.keys():\n",
    "        correct_images = mixed_images_dict[class_idx]['correct']\n",
    "        incorrect_images = mixed_images_dict[class_idx]['incorrect']\n",
    "        \n",
    "        # If there are not enough correct or incorrect images, fill in with blank images\n",
    "        while len(correct_images) < 5:\n",
    "            correct_images.append(torch.zeros_like(correct_images[0]))\n",
    "        while len(incorrect_images) < 5:\n",
    "            incorrect_images.append(torch.zeros_like(correct_images[0]))\n",
    "        \n",
    "        # Combine correct and incorrect images into a single image\n",
    "        combined_image = torch.cat([torch.stack(correct_images), torch.stack(incorrect_images)], dim=0)\n",
    "        combined_images.append(combined_image)\n",
    "    \n",
    "    return combined_images\n",
    "\n",
    "# Example usage\n",
    "combined_images_per_class = create_combined_image_per_class(mixed_images_dict)\n",
    "\n",
    "# Plot combined images for each class\n",
    "fig, axs = plt.subplots(2, 5, figsize=(20, 8))\n",
    "for i, combined_image in enumerate(combined_images_per_class):\n",
    "    axs[i//5, i%5].imshow(vutils.make_grid(combined_image, nrow=5, padding=2, normalize=True).permute(1, 2, 0))\n",
    "    axs[i//5, i%5].axis('off')\n",
    "    axs[i//5, i%5].set_title(f\"Class {NUM_2_CLASS[i]}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"plots/predict.png\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images_in_folder(folder_path):\n",
    "    # Get a list of all files in the folder\n",
    "    files = os.listdir(folder_path)\n",
    "    \n",
    "    # Filter out only image files\n",
    "    image_files = [f for f in files if f.endswith(('.png', '.jpg', '.jpeg', '.gif'))]\n",
    "    \n",
    "    # Plot each image\n",
    "    for image_file in image_files:\n",
    "        if image_file[:2] == 'lr':\n",
    "            # Load the image\n",
    "            image_path = os.path.join(folder_path, image_file)\n",
    "            image = Image.open(image_path)\n",
    "            \n",
    "            # Plot the image\n",
    "            plt.figure()\n",
    "            plt.imshow(image)\n",
    "            plt.title(image_file)\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "\n",
    "# Path to your plots folder\n",
    "plots_folder_path = 'plots'\n",
    "\n",
    "# Call the function to plot images in the folder\n",
    "plot_images_in_folder(plots_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = mpimg.imread(\"plots/best_train.png\")\n",
    "plt.imshow(image)\n",
    "plt.axis('off') \n",
    "plt.show()\n",
    "\n",
    "print(\"best epoch - 16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = mpimg.imread(\"plots/predict.png\")\n",
    "plt.imshow(image)\n",
    "plt.axis('off') \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"classification/data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the relevant columns\n",
    "real_labels = df['real_label']\n",
    "pred_labels = df['pred_label']\n",
    "train_labels = df['train']\n",
    "\n",
    "# Compute the confusion matrix for both train and test\n",
    "conf_matrix_train = confusion_matrix(real_labels[train_labels], pred_labels[train_labels])\n",
    "conf_matrix_test = confusion_matrix(real_labels[~train_labels], pred_labels[~train_labels])\n",
    "\n",
    "# Plot the confusion matrix for train data\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix_train, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "plt.title('Confusion Matrix (Train)')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()\n",
    "\n",
    "# Plot the confusion matrix for test data\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix_test, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "plt.title('Confusion Matrix (Test)')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_metrics(true_labels, predicted_labels, target_class =0 ):\n",
    "    # Calculate confusion matrix\n",
    "    conf_matrix = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "    true_positives = np.diag(conf_matrix)\n",
    "    false_positives = np.sum(conf_matrix, axis=0) - true_positives\n",
    "    false_negatives = np.sum(conf_matrix, axis=1) - true_positives\n",
    "    true_negatives = np.sum(conf_matrix) - (true_positives + false_positives + false_negatives)\n",
    "    \n",
    "    # Accuracy\n",
    "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "    \n",
    "    # Precision\n",
    "    precision = precision_score(true_labels, predicted_labels, average='macro')\n",
    "    \n",
    "    # Recall\n",
    "    recall = recall_score(true_labels, predicted_labels, average='macro')\n",
    "    \n",
    "    # F1-score\n",
    "    f1 = f1_score(true_labels, predicted_labels, average='macro')\n",
    "    \n",
    "    \n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "\n",
    "# Extract true and predicted labels from the DataFrame\n",
    "true_labels_train = df[df['train']]['real_label']\n",
    "predicted_labels_train = df[df['train']]['pred_label']\n",
    "true_labels_test = df[~df['train']]['real_label']\n",
    "predicted_labels_test = df[~df['train']]['pred_label']\n",
    "\n",
    "# Calculate metrics for train set\n",
    "accuracy_train, precision_train, recall_train, f1_train = evaluate_metrics(true_labels_train, predicted_labels_train)\n",
    "\n",
    "# Calculate metrics for test set\n",
    "accuracy_test, precision_test, recall_test, f1_test = evaluate_metrics(true_labels_test, predicted_labels_test)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Train Set Metrics:\")\n",
    "print(f\"Accuracy: {accuracy_train:.4f}\")\n",
    "print(f\"Precision: {precision_train:.4f}\")\n",
    "print(f\"Recall: {recall_train:.4f}\")\n",
    "print(f\"F1-score: {f1_train:.4f}\")\n",
    "\n",
    "\n",
    "print(\"Test Set Metrics:\")\n",
    "print(f\"Accuracy: {accuracy_test:.4f}\")\n",
    "print(f\"Precision: {precision_test:.4f}\")\n",
    "print(f\"Recall: {recall_test:.4f}\")\n",
    "print(f\"F1-score: {f1_test:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def one_vs_all_precision_recall(true_labels, predicted_labels, num_classes):\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "\n",
    "    for class_label in range(num_classes):\n",
    "        # Treat class_label as the positive class and all other classes as negative\n",
    "        true_labels_positive = (true_labels == class_label)\n",
    "        predicted_labels_positive = (predicted_labels == class_label)\n",
    "\n",
    "        precision = precision_score(true_labels_positive, predicted_labels_positive)\n",
    "        recall = recall_score(true_labels_positive, predicted_labels_positive)\n",
    "\n",
    "        precision_scores.append(precision)\n",
    "        recall_scores.append(recall)\n",
    "\n",
    "    return precision_scores, recall_scores\n",
    "\n",
    "# Assuming you have true labels and predicted labels for the test set\n",
    "# Replace these placeholders with your actual labels\n",
    "true_labels_test = df[~df['train']]['real_label']\n",
    "predicted_labels_test = df[~df['train']]['pred_label']\n",
    "\n",
    "# Calculate precision and recall for one-vs-all approach\n",
    "num_classes = len(df['real_label'].unique())  # Number of unique classes in the dataset\n",
    "precision_scores, recall_scores = one_vs_all_precision_recall(true_labels_test, predicted_labels_test, num_classes)\n",
    "\n",
    "# Plot precision and recall together\n",
    "plt.figure(figsize=(10, 6))\n",
    "for class_label in range(num_classes):\n",
    "    plt.plot(recall_scores[class_label], precision_scores[class_label], marker='o', linestyle='-', label=f'Class {NUM_2_CLASS[class_label]}')\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve for One-vs-All')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder Anomaly Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from autoencoder import *\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    print (\"MPS device not found.\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "dataset = AutoencoderDataSet(n_samples_from_each_class=26, transform=transform)\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = dataset.get_splits(train_size=0.7, val_size=0.3)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "autoencoder_results_folder = 'results/autoencoder/'\n",
    "fig, axs = plt.subplots(2, 2, figsize=(15, 10))\n",
    "axs = axs.flatten()  \n",
    "\n",
    "for ax, img_name in zip(axs, os.listdir(autoencoder_results_folder)):\n",
    "    img_path = os.path.join(autoencoder_results_folder, img_name)\n",
    "    img = Image.open(img_path)\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(img_name)\n",
    "    ax.axis('off')  # Hide axes ticks\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = Autoencoder().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, betas=(0.95, 0.999))\n",
    "\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, images)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, images)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch {epoch+1}, Validation Loss: {val_loss/len(val_loader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'autoencoder_weights/best_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Autoencoder()\n",
    "model.load_state_dict(torch.load('autoencoder_weights/best_model.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "images, _ = next(iter(val_loader))\n",
    "images = images.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    reconstructions = model(images)\n",
    "\n",
    "visualize_reconstructions(images, reconstructions, n=10)  # Feel free to adjust n\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, _ = next(iter(test_loader))\n",
    "images = images.to(device)\n",
    "with torch.no_grad():\n",
    "    reconstructions = model(images)\n",
    "\n",
    "visualize_reconstructions(images, reconstructions, n=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoencoder import generate_output_dataframe\n",
    "mse_df = generate_output_dataframe(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,8))\n",
    "sns.boxplot(mse_df, x='label', y='scaled_rgb_mse' ,hue='label')\n",
    "plt.title(\"MSE per label with RGB scaled to 0-1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,8))\n",
    "sns.boxplot(mse_df, x='label', y='scaled_hsv_mse' ,hue='label')\n",
    "plt.title(\"MSE per label with HSV scaled to 0-1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,8))\n",
    "sns.boxplot(mse_df, x='label', y='non_scaled_hsv_mse' ,hue='label')\n",
    "plt.title(\"MSE per label with HSV without scaling\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_df['binary_label'] = mse_df['label'].apply(lambda x: 0 if x==0 else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "healthy_25_percentile = mse_df.query(\"label == 0\").non_scaled_hsv_mse.quantile(0.25)\n",
    "healthy_75_percentile = mse_df.query(\"label == 0\").non_scaled_hsv_mse.quantile(0.75)\n",
    "\n",
    "print(\"0.25 percentile: \", healthy_25_percentile)\n",
    "print(\"0.75 percentile: \", healthy_75_percentile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_scores = mse_df['non_scaled_hsv_mse']\n",
    "true_labels = mse_df['binary_label']\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(true_labels, mse_scores)\n",
    "\n",
    "# Calculate F1 scores\n",
    "f1_scores = 2 * (precision[:-1] * recall[:-1]) / (precision[:-1] + recall[:-1])\n",
    "\n",
    "# Find the index of the maximum F1 score\n",
    "max_f1_index = np.argmax(f1_scores)\n",
    "# Corresponding best threshold\n",
    "best_threshold = thresholds[max_f1_index]\n",
    "\n",
    "# Plot the precision-recall curve\n",
    "plt.plot(recall, precision, label='Precision-Recall Curve')\n",
    "plt.scatter(recall[max_f1_index+1], precision[max_f1_index+1], color='red', label=f'Best Threshold: {best_threshold:.2f}')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f'Best Threshold: {best_threshold}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Vector Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import KNeighborsClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_loader(model, loader):\n",
    "    x = []\n",
    "    y = []\n",
    "    for images, labels in loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        with torch.no_grad():\n",
    "            embedding = model.encode(images)\n",
    "            y += [label.item() for label in labels]\n",
    "        x.append(embedding) \n",
    "    return torch.cat(x, dim=0), y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embeddings, train_labels = encode_loader(model, train_loader)\n",
    "test_embeddings, test_labels = encode_loader(model, test_loader)\n",
    "train_embeddings.shape, test_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_train_flat = train_embeddings.reshape(train_embeddings.shape[0], -1)\n",
    "encoded_test_flat = test_embeddings.reshape(test_embeddings.shape[0], -1)\n",
    "encoded_train_flat.shape, encoded_test_flat.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tsne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import plotly.graph_objs as go\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_test_flat_np = encoded_test_flat.cpu().detach().numpy()\n",
    "tsne = TSNE(n_components=3)\n",
    "embedded_data = tsne.fit_transform(encoded_test_flat_np)\n",
    "test_labels_np = np.array(test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a trace for the point cloud\n",
    "trace = go.Scatter3d(\n",
    "    x=embedded_data[:, 0],\n",
    "    y=embedded_data[:, 1],\n",
    "    z=embedded_data[:, 2],\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size=5,\n",
    "        color=[(0, 0, 1) if label == 0 else (1, 0, 0) for label in test_labels_np],  # Blue for 0, red for other labels\n",
    "        opacity=0.8\n",
    "    )\n",
    ")\n",
    "\n",
    "# Create layout\n",
    "layout = go.Layout(\n",
    "    scene=dict(\n",
    "        xaxis=dict(title='Dimension 1'),\n",
    "        yaxis=dict(title='Dimension 2'),\n",
    "        zaxis=dict(title='Dimension 3')\n",
    "    )\n",
    ")\n",
    "\n",
    "# Create figure\n",
    "fig = go.Figure(data=[trace], layout=layout)\n",
    "\n",
    "# Show plot\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_train_flat_np = encoded_train_flat.cpu().detach().numpy()\n",
    "\n",
    "mean_vector = np.mean(encoded_train_flat_np, axis=0)\n",
    "\n",
    "print(\"Mean vector:\", mean_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_test_flat_np = encoded_test_flat.cpu().detach().numpy()\n",
    "distances = np.linalg.norm(encoded_test_flat_np - mean_vector, axis=1)\n",
    "\n",
    "# Print the distances\n",
    "print(\"Distances shape:\", distances.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels_np = np.array(test_labels)\n",
    "\n",
    "dict_distances = {\"dist\" : distances , \"class\" : test_labels_np}\n",
    "df = pd.DataFrame(dict_distances)\n",
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,8))\n",
    "sns.boxplot(df, x='class', y='dist' ,hue='class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "# Assuming you have distances and test_labels_np already defined\n",
    "\n",
    "# Define true labels for binary classification (class 0 vs all others)\n",
    "true_labels_binary = (test_labels_np == 0).astype(int)  # 1 for class 0, 0 for all other classes\n",
    "\n",
    "# Initialize lists to store precision, recall, and F1 score for each threshold\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "\n",
    "# Define thresholds from the lowest distance in class 0 to the highest distance in class 0\n",
    "lowest_dist_class_0 = min(distances[test_labels_np == 0])\n",
    "highest_dist_class_0 = max(distances[test_labels_np == 0])\n",
    "thresholds = np.linspace(lowest_dist_class_0, highest_dist_class_0, num=20)\n",
    "\n",
    "# Compute precision, recall, and F1 score for each threshold\n",
    "for threshold in thresholds:\n",
    "    # Convert distances to predicted labels (1 if distance is smaller or equal to threshold, 0 otherwise)\n",
    "    predicted_labels_binary = (distances <= threshold).astype(int)\n",
    "    \n",
    "    # Compute precision, recall, and F1 score\n",
    "    precision = precision_score(true_labels_binary, predicted_labels_binary)\n",
    "    recall = recall_score(true_labels_binary, predicted_labels_binary)\n",
    "    f1 = f1_score(true_labels_binary, predicted_labels_binary)\n",
    "    \n",
    "    # Append precision, recall, and F1 score to lists\n",
    "    precision_scores.append(precision)\n",
    "    recall_scores.append(recall)\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "# Find the threshold that maximizes the F1 score\n",
    "best_f1_index = np.argmax(f1_scores)\n",
    "best_threshold = thresholds[best_f1_index]\n",
    "best_precision = precision_scores[best_f1_index]\n",
    "best_recall = recall_scores[best_f1_index]\n",
    "best_f1 = f1_scores[best_f1_index]\n",
    "\n",
    "# Print the best scores\n",
    "print(\"Best Threshold:\", best_threshold)\n",
    "print(\"Best Precision:\", best_precision)\n",
    "print(\"Best Recall:\", best_recall)\n",
    "print(\"Best F1 Score:\", best_f1)\n",
    "\n",
    "# Plot recall vs precision\n",
    "plt.plot(recall_scores, precision_scores, marker='o', linestyle='-')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.grid(True)\n",
    "\n",
    "# Mark threshold points on the graph\n",
    "for i, threshold in enumerate(thresholds):\n",
    "    plt.text(recall_scores[i], precision_scores[i], f\"Th: {threshold:.2f}\", fontsize=8)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_np = train_embeddings.numpy()\n",
    "\n",
    "n_clusters = 2\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "kmeans.fit(encoded_test_flat)\n",
    "\n",
    "# The cluster assignment for each data point\n",
    "cluster_labels = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump\n",
    "model_filename = 'kmeans_model.joblib'\n",
    "dump(kmeans, model_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_neighbors = 5\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=n_neighbors)\n",
    "knn.fit(encoded_train_flat, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = knn.predict(encoded_test_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
