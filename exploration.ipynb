{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import subprocess\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import matplotlib.image as mpimg\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "from classification_utils import *\n",
    "from train_classification import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> This Code creates DataFrame with the columns:\n",
    "- classfication (name of the image folder)\n",
    "- image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Path to the dataset directory\n",
    "dataset_dir = 'data/PlantVillage/'\n",
    "\n",
    "# Function to load images and labels using OpenCV\n",
    "def load_dataset(dataset_path):\n",
    "    data_dict = {}\n",
    "    # Walk through the dataset directory\n",
    "    for root, dirs, files in os.walk(dataset_path):\n",
    "        for dir_name in dirs:\n",
    "            # Initialize an empty list for each subfolder\n",
    "            data_dict[dir_name] = []\n",
    "\n",
    "            # Get the full path to the subfolder\n",
    "            subfolder_path = os.path.join(root, dir_name)\n",
    "\n",
    "            # Iterate through the files in the subfolder\n",
    "            for file in os.listdir(subfolder_path):\n",
    "                if file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                    # Construct the full path to the image\n",
    "                    image_path = os.path.join(subfolder_path, file)\n",
    "\n",
    "                    # Load the image using OpenCV\n",
    "                    image = cv2.imread(image_path)\n",
    "                    # Convert from BGR to RGB color space\n",
    "                    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "                    # Append the image to the list associated with the subfolder name\n",
    "                    data_dict[dir_name].append(image)\n",
    "    data = [(classification, path) for classification, paths in data_dict.items() for path in paths]\n",
    "    df = pd.DataFrame(data, columns=['classification', 'image'])\n",
    "    return df\n",
    "\n",
    "# Load your dataset\n",
    "df = load_dataset(dataset_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Lets keep the tomatos images only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in df.classification.unique():\n",
    "    if 'tomato' in label.lower():\n",
    "        print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tomato_data = df.loc[df['classification'].str.contains('Tomato')]\n",
    "tomato_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Sample from each classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_images_per_class = 4\n",
    "images_to_display = []\n",
    "titles = []\n",
    "for label in tomato_data.classification.unique():\n",
    "    label_sample = tomato_data.loc[tomato_data.classification == label].sample(num_images_per_class).image.to_list()\n",
    "    images_to_display += label_sample\n",
    "    titles += [label]*num_images_per_class\n",
    "\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(nrows=len(tomato_data.classification.unique()), ncols=num_images_per_class, figsize=(15, 20))  \n",
    "for ax, image, title in zip(axes.flatten(), images_to_display, titles):\n",
    "    ax.imshow(image)\n",
    "    ax.set_title(title)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax, fig = plt.subplots(figsize=(15,7))\n",
    "sns.barplot(tomato_data.classification.value_counts().reset_index(), y='index', x='classification')\n",
    "plt.yticks(fontsize=14)\n",
    "plt.title(\"Number of images per classification\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Check weather the images resolution are unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tomato_data['image_shape'] = tomato_data.image.apply(lambda x: x.shape)\n",
    "    \n",
    "print(f\"The image resolution we have in the dataset are : {tomato_data.image_shape.unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmented_data_root_folder = 'data/segmented_data/'\n",
    "number_of_images_to_segment = 300\n",
    "\n",
    "for root, dirs, files in os.walk(dataset_dir):\n",
    "    for dir_name in dirs:\n",
    "        if 'tomato' in dir_name.lower():\n",
    "            subfolder_path = os.path.join(root, dir_name)\n",
    "            destination_directory = os.path.join(segmented_data_root_folder, dir_name)\n",
    "            os.makedirs(destination_directory, exist_ok=True)\n",
    "            \n",
    "            dir_files = os.listdir(subfolder_path)\n",
    "            random_files = random.sample(dir_files, min(number_of_images_to_segment, len(dir_files)))\n",
    "        else: continue\n",
    "        for file in random_files:\n",
    "            if file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                image_path = os.path.join(subfolder_path, file)\n",
    "                command = [\n",
    "                        \"python3\", \"leaf-image-segmentation/segment.py\",\n",
    "                        \"-d\", destination_directory,\n",
    "                        \"-f\", \"flood\",  \n",
    "                        image_path\n",
    "                    ]\n",
    "                subprocess.run(command)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmented_data = load_dataset('data/segmented_data/')\n",
    "segmented_data_dict = [(classification, image) for classification, images in segmented_data.items() for image in images]\n",
    "segmented_data_df = pd.DataFrame(segmented_data_dict, columns=['classification', 'image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_images_per_class = 4\n",
    "images_to_display = []\n",
    "titles = []\n",
    "for label in segmented_data_df.classification.unique():\n",
    "    label_sample = segmented_data_df.loc[segmented_data_df.classification == label].sample(num_images_per_class).image.to_list()\n",
    "    images_to_display += label_sample\n",
    "    titles += [label]*num_images_per_class\n",
    "\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(nrows=len(segmented_data_df.classification.unique()), ncols=num_images_per_class, figsize=(15, 20))  \n",
    "for ax, image, title in zip(axes.flatten(), images_to_display, titles):\n",
    "    ax.imshow(image)\n",
    "    ax.set_title(title)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_rgb_to_hsv(rgb_image):\n",
    "    # Convert the RGB image to HSV\n",
    "    return cv2.cvtColor(rgb_image, cv2.COLOR_RGB2HSV)\n",
    "\n",
    "# Apply the conversion function to each image in the DataFrame\n",
    "segmented_data_df['hsv_image'] = segmented_data_df['image'].apply(convert_rgb_to_hsv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(segmented_data_df.hsv_image.iloc[600])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(segmented_data_df.hsv_image.iloc[600])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmented_data_df.hsv_image.iloc.reshape(-1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "pixels = segmented_data_df.hsv_image.iloc[0].reshape(-1, 3)\n",
    "kmeans = KMeans(n_clusters=3)\n",
    "kmeans.fit(pixels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tomato Disease Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "b1 = 0.95\n",
    "b2 = 0.99\n",
    "num_epochs = 20\n",
    "train_data ,val_data, test_data = get_dataloader(dataset_dir = 'data/PlantVillage/' , target_count = 2200 ,batch_size = 75 ,show = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to save the model weights\n",
    "weights_folder = 'weights_classification'\n",
    "if not os.path.exists(weights_folder):\n",
    "    os.makedirs(weights_folder)\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    print (\"MPS device not found.\")\n",
    "print(device)\n",
    "beta = (b1, b2)\n",
    "model = CNN(num_classes=10)\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, betas=beta)\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "for epoch in tqdm(range(num_epochs), desc=\"Training\"):\n",
    "    train_loss, train_accuracy = train(model, train_data, criterion, optimizer, device)\n",
    "    val_loss, val_accuracy = validate(model, val_data, criterion, device)\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "\n",
    "    model_weights_path = os.path.join(weights_folder, f\"epoch_{epoch+1}_weights.pth\")\n",
    "    torch.save(model.state_dict(), model_weights_path)\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], lr={lr}, beta={beta}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = len(train_losses)\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, epochs + 1), train_losses, label='Train Loss')\n",
    "plt.plot(range(1, epochs + 1), val_losses, label='Validation Loss')\n",
    "plt.title(f'Loss (lr={lr}, beta={beta})')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, epochs + 1), train_accuracies, label='Train Accuracy')\n",
    "plt.plot(range(1, epochs + 1), val_accuracies, label='Validation Accuracy')\n",
    "plt.title(f'Accuracy (lr={lr}, beta={beta})')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'plots/best_train.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_epoch = 16\n",
    "model = CNN(num_classes=10)\n",
    "model.to(device)\n",
    "weights_path = f\"weights_classification/epoch_{best_epoch}_weights.pth\"\n",
    "\n",
    "# Load the weights\n",
    "model.load_state_dict(torch.load(weights_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create an empty list to store combined data\n",
    "combined_data_list = []\n",
    "\n",
    "# Iterate through the test_data\n",
    "for images, labels in test_data:\n",
    "    # Make predictions (you need to adjust this part based on how you make predictions)\n",
    "    predictions = model(images.to(device))\n",
    "    _, predicted_labels = torch.max(predictions, 1)\n",
    "\n",
    "    # Append data to the combined list\n",
    "    for real_label, pred_label in zip(labels, predicted_labels):\n",
    "        combined_data_list.append({'real_label': real_label.item(),\n",
    "                                   'pred_label': pred_label.item(),\n",
    "                                   'train': False})  # This is test data\n",
    "\n",
    "# Iterate through the train_data\n",
    "for images, labels in train_data:\n",
    "    # Make predictions (you need to adjust this part based on how you make predictions)\n",
    "    predictions = model(images.to(device))\n",
    "    _, predicted_labels = torch.max(predictions, 1)\n",
    "\n",
    "    # Append data to the combined list\n",
    "    for real_label, pred_label in zip(labels, predicted_labels):\n",
    "        combined_data_list.append({'real_label': real_label.item(),\n",
    "                                   'pred_label': pred_label.item(),\n",
    "                                   'train': True})  # This is train data\n",
    "\n",
    "# Create a DataFrame from the combined list\n",
    "combined_df = pd.DataFrame(combined_data_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.to_csv('classification/data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mixed_image_per_class(model, data_loader, num_images_per_class=5):\n",
    "    mixed_images = {}\n",
    "    \n",
    "    # Initialize the dictionary for each class\n",
    "    for class_idx in range(10):  \n",
    "        mixed_images[class_idx] = {'correct': [], 'incorrect': []}\n",
    "    \n",
    "    # Iterate through each batch in the data loader\n",
    "    for images, labels in data_loader:\n",
    "        # Make predictions\n",
    "        predictions = model(images.to(device))\n",
    "        _, predicted_labels = torch.max(predictions, 1)\n",
    "        \n",
    "        # Iterate through the batch\n",
    "        for image, true_label, pred_label in zip(images, labels, predicted_labels):\n",
    "            class_idx = true_label.item()\n",
    "            if true_label == pred_label:\n",
    "                # Append to correct predictions if less than num_images_per_class\n",
    "                if len(mixed_images[class_idx]['correct']) < num_images_per_class:\n",
    "                    mixed_images[class_idx]['correct'].append(image)\n",
    "            else:\n",
    "                # Append to incorrect predictions if less than num_images_per_class\n",
    "                if len(mixed_images[class_idx]['incorrect']) < num_images_per_class:\n",
    "                    mixed_images[class_idx]['incorrect'].append(image)\n",
    "            \n",
    "            # # Break if we have enough examples for both correct and incorrect predictions for the current class\n",
    "            # if len(mixed_images[class_idx]['correct']) >= num_images_per_class and len(mixed_images[class_idx]['incorrect']) >= num_images_per_class:\n",
    "            #     break\n",
    "                \n",
    "    return mixed_images\n",
    "\n",
    "# Example usage\n",
    "mixed_images_dict = create_mixed_image_per_class(model, test_data, num_images_per_class=5)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "def create_combined_image_per_class(mixed_images_dict):\n",
    "    combined_images = []\n",
    "    \n",
    "    for class_idx in mixed_images_dict.keys():\n",
    "        correct_images = mixed_images_dict[class_idx]['correct']\n",
    "        incorrect_images = mixed_images_dict[class_idx]['incorrect']\n",
    "        \n",
    "        # If there are not enough correct or incorrect images, fill in with blank images\n",
    "        while len(correct_images) < 5:\n",
    "            correct_images.append(torch.zeros_like(correct_images[0]))\n",
    "        while len(incorrect_images) < 5:\n",
    "            incorrect_images.append(torch.zeros_like(correct_images[0]))\n",
    "        \n",
    "        # Combine correct and incorrect images into a single image\n",
    "        combined_image = torch.cat([torch.stack(correct_images), torch.stack(incorrect_images)], dim=0)\n",
    "        combined_images.append(combined_image)\n",
    "    \n",
    "    return combined_images\n",
    "\n",
    "# Example usage\n",
    "combined_images_per_class = create_combined_image_per_class(mixed_images_dict)\n",
    "\n",
    "# Plot combined images for each class\n",
    "fig, axs = plt.subplots(2, 5, figsize=(20, 8))\n",
    "for i, combined_image in enumerate(combined_images_per_class):\n",
    "    axs[i//5, i%5].imshow(vutils.make_grid(combined_image, nrow=5, padding=2, normalize=True).permute(1, 2, 0))\n",
    "    axs[i//5, i%5].axis('off')\n",
    "    axs[i//5, i%5].set_title(f\"Class {NUM_2_CLASS[i]}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"plots/predict.png\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images_in_folder(folder_path):\n",
    "    # Get a list of all files in the folder\n",
    "    files = os.listdir(folder_path)\n",
    "    \n",
    "    # Filter out only image files\n",
    "    image_files = [f for f in files if f.endswith(('.png', '.jpg', '.jpeg', '.gif'))]\n",
    "    \n",
    "    # Plot each image\n",
    "    for image_file in image_files:\n",
    "        if image_file[:2] == 'lr':\n",
    "            # Load the image\n",
    "            image_path = os.path.join(folder_path, image_file)\n",
    "            image = Image.open(image_path)\n",
    "            \n",
    "            # Plot the image\n",
    "            plt.figure()\n",
    "            plt.imshow(image)\n",
    "            plt.title(image_file)\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "\n",
    "# Path to your plots folder\n",
    "plots_folder_path = 'plots'\n",
    "\n",
    "# Call the function to plot images in the folder\n",
    "plot_images_in_folder(plots_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = mpimg.imread(\"plots/best_train.png\")\n",
    "plt.imshow(image)\n",
    "plt.axis('off') \n",
    "plt.show()\n",
    "\n",
    "print(\"best epoch - 16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = mpimg.imread(\"plots/predict.png\")\n",
    "plt.imshow(image)\n",
    "plt.axis('off') \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"classification/data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the relevant columns\n",
    "real_labels = df['real_label']\n",
    "pred_labels = df['pred_label']\n",
    "train_labels = df['train']\n",
    "\n",
    "# Compute the confusion matrix for both train and test\n",
    "conf_matrix_train = confusion_matrix(real_labels[train_labels], pred_labels[train_labels])\n",
    "conf_matrix_test = confusion_matrix(real_labels[~train_labels], pred_labels[~train_labels])\n",
    "\n",
    "# Plot the confusion matrix for train data\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix_train, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "plt.title('Confusion Matrix (Train)')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()\n",
    "\n",
    "# Plot the confusion matrix for test data\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix_test, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "plt.title('Confusion Matrix (Test)')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_metrics(true_labels, predicted_labels, target_class =0 ):\n",
    "    # Calculate confusion matrix\n",
    "    conf_matrix = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "    true_positives = np.diag(conf_matrix)\n",
    "    false_positives = np.sum(conf_matrix, axis=0) - true_positives\n",
    "    false_negatives = np.sum(conf_matrix, axis=1) - true_positives\n",
    "    true_negatives = np.sum(conf_matrix) - (true_positives + false_positives + false_negatives)\n",
    "    \n",
    "    # Accuracy\n",
    "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "    \n",
    "    # Precision\n",
    "    precision = precision_score(true_labels, predicted_labels, average='macro')\n",
    "    \n",
    "    # Recall\n",
    "    recall = recall_score(true_labels, predicted_labels, average='macro')\n",
    "    \n",
    "    # F1-score\n",
    "    f1 = f1_score(true_labels, predicted_labels, average='macro')\n",
    "    \n",
    "    \n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "\n",
    "# Extract true and predicted labels from the DataFrame\n",
    "true_labels_train = df[df['train']]['real_label']\n",
    "predicted_labels_train = df[df['train']]['pred_label']\n",
    "true_labels_test = df[~df['train']]['real_label']\n",
    "predicted_labels_test = df[~df['train']]['pred_label']\n",
    "\n",
    "# Calculate metrics for train set\n",
    "accuracy_train, precision_train, recall_train, f1_train = evaluate_metrics(true_labels_train, predicted_labels_train)\n",
    "\n",
    "# Calculate metrics for test set\n",
    "accuracy_test, precision_test, recall_test, f1_test = evaluate_metrics(true_labels_test, predicted_labels_test)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Train Set Metrics:\")\n",
    "print(f\"Accuracy: {accuracy_train:.4f}\")\n",
    "print(f\"Precision: {precision_train:.4f}\")\n",
    "print(f\"Recall: {recall_train:.4f}\")\n",
    "print(f\"F1-score: {f1_train:.4f}\")\n",
    "\n",
    "\n",
    "print(\"Test Set Metrics:\")\n",
    "print(f\"Accuracy: {accuracy_test:.4f}\")\n",
    "print(f\"Precision: {precision_test:.4f}\")\n",
    "print(f\"Recall: {recall_test:.4f}\")\n",
    "print(f\"F1-score: {f1_test:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def one_vs_all_precision_recall(true_labels, predicted_labels, num_classes):\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "\n",
    "    for class_label in range(num_classes):\n",
    "        # Treat class_label as the positive class and all other classes as negative\n",
    "        true_labels_positive = (true_labels == class_label)\n",
    "        predicted_labels_positive = (predicted_labels == class_label)\n",
    "\n",
    "        precision = precision_score(true_labels_positive, predicted_labels_positive)\n",
    "        recall = recall_score(true_labels_positive, predicted_labels_positive)\n",
    "\n",
    "        precision_scores.append(precision)\n",
    "        recall_scores.append(recall)\n",
    "\n",
    "    return precision_scores, recall_scores\n",
    "\n",
    "# Assuming you have true labels and predicted labels for the test set\n",
    "# Replace these placeholders with your actual labels\n",
    "true_labels_test = df[~df['train']]['real_label']\n",
    "predicted_labels_test = df[~df['train']]['pred_label']\n",
    "\n",
    "# Calculate precision and recall for one-vs-all approach\n",
    "num_classes = len(df['real_label'].unique())  # Number of unique classes in the dataset\n",
    "precision_scores, recall_scores = one_vs_all_precision_recall(true_labels_test, predicted_labels_test, num_classes)\n",
    "\n",
    "# Plot precision and recall together\n",
    "plt.figure(figsize=(10, 6))\n",
    "for class_label in range(num_classes):\n",
    "    plt.plot(recall_scores[class_label], precision_scores[class_label], marker='o', linestyle='-', label=f'Class {NUM_2_CLASS[class_label]}')\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve for One-vs-All')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder Anomaly Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from autoencoder import AutoencoderDataSet, Autoencoder, generate_output_dataframe\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "dataset = AutoencoderDataSet(n_samples_from_each_class=300, transform=transform)\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = dataset.get_splits(train_size=0.7, val_size=0.3)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "autoencoder_results_folder = 'results/autoencoder/'\n",
    "fig, axs = plt.subplots(2, 2, figsize=(15, 10))\n",
    "axs = axs.flatten()  \n",
    "\n",
    "for ax, img_name in zip(axs, os.listdir(autoencoder_results_folder)):\n",
    "    img_path = os.path.join(autoencoder_results_folder, img_name)\n",
    "    img = Image.open(img_path)\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(img_name)\n",
    "    ax.axis('off')  # Hide axes ticks\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Autoencoder().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, betas=(0.95, 0.999))\n",
    "\n",
    "num_epochs = 30\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, images)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, images)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch {epoch+1}, Validation Loss: {val_loss/len(val_loader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoencoder import visualize_reconstructions\n",
    "model.eval()\n",
    "\n",
    "images, _ = next(iter(val_loader))\n",
    "images = images.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    reconstructions = model(images)\n",
    "\n",
    "visualize_reconstructions(images, reconstructions, n=10)  # Feel free to adjust n\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, _ = next(iter(test_loader))\n",
    "images = images.to(device)\n",
    "with torch.no_grad():\n",
    "    reconstructions = model(images)\n",
    "\n",
    "visualize_reconstructions(images, reconstructions, n=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoencoder import generate_output_dataframe\n",
    "mse_df = generate_output_dataframe(model, test_loader, hsv_evaluation=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(mse_df, x='mse')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(15,8))\n",
    "sns.boxplot(mse_df, x='label', y='mse', hue='label')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
